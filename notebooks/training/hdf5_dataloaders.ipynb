{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZeJ6pxiGYn1"
   },
   "source": [
    "# Using dataloaders with HDF5 files in torch\n",
    "\n",
    "Example of how to use `h5py` and `torch.utils.data.Dataset` to load data in batches without loading a whole file.\n",
    "\n",
    "**Note:** this will often be slower than loading all of the data at once, but it avoids having to load large files in memory which can cause issues on shared compute resources.\n",
    "\n",
    "Michael J. Williams 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIHqmdQO64Op"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b9pXVN3nAfo",
    "outputId": "58e63ab8-a8c4-42e9-8238-5d8615de5cdd"
   },
   "outputs": [],
   "source": [
    "np.__path__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vortey7_OJtu"
   },
   "source": [
    "# Dataset class\n",
    "\n",
    "This class is designed for use in a supervised learning problem but can easily be adapted for unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaLpjTaj7D-i"
   },
   "outputs": [],
   "source": [
    "class H5Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A dataset to handle large HDF5 files.\n",
    "\n",
    "    Based on this post:\n",
    "        https://discuss.pytorch.org/t/efficiently-saving-and-loading-data-using-h5py-or-other-methods/74153\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    h5_path\n",
    "      Path to the HDF5 file\n",
    "    x_key\n",
    "      Key for the x data\n",
    "    y_key\n",
    "      Key for the y data\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        h5_path: str,\n",
    "        x_key: str,\n",
    "        y_key: str,\n",
    "      ) -> None:\n",
    "\n",
    "        self.h5_path = h5_path\n",
    "        self._h5_gen = None\n",
    "        self.x_key = x_key\n",
    "        self.y_key = y_key\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if self._h5_gen is None:\n",
    "            self._h5_gen = self._get_generator()\n",
    "            next(self._h5_gen)\n",
    "        return self._h5_gen.send(index)\n",
    "\n",
    "    def _get_generator(self):\n",
    "        with h5py.File( self.h5_path, 'r') as record:\n",
    "            index = yield\n",
    "            while True:\n",
    "                X = record[self.x_key][index]\n",
    "                y = record[self.y_key][index]\n",
    "                index = yield X, y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        with h5py.File(self.h5_path,'r') as record:\n",
    "            length = record[self.x_key].shape[0]\n",
    "            return length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huDtX0JSGzwh"
   },
   "source": [
    "# Make an example HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pa8s61KQEKTU"
   },
   "outputs": [],
   "source": [
    "filename = 'test.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4S41aidIKgE"
   },
   "source": [
    "Let's check the RAM usage before we create the large dataset. The values here are printed in `MB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQuAVp8yIAYq",
    "outputId": "136b9ba0-4361-4f5b-a999-39677333c521"
   },
   "outputs": [],
   "source": [
    "!free -m | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gimndc65IOPV"
   },
   "source": [
    "Now let's create the large dataset.\n",
    "\n",
    "In a real use case you would do this in a seperate script and only load the data in the training script. You could also make use of the option to add data in batches to the file, so you don't have to generate it all at once.\n",
    "\n",
    "See: https://docs.h5py.org/en/stable/high/dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Kl952ApD7Hk"
   },
   "outputs": [],
   "source": [
    "N = 100_000\n",
    "x_data = np.random.randn(N, 1000)\n",
    "y_data = np.random.randn(N, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH1HQzjsJOc8"
   },
   "source": [
    "Now save the data into a HDF5 file using `h5py`. We'll use one dataset for the x data and another for the y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4skd9B1EJmH"
   },
   "outputs": [],
   "source": [
    "with h5py.File(filename, 'w') as f:\n",
    "    f.create_dataset('time_series', data=x_data, dtype='float32')\n",
    "    f.create_dataset('targets', data=y_data, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJJg_AFnG5Ww"
   },
   "source": [
    "We can check the size of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVADNjpOEeg6",
    "outputId": "0fdee88d-005f-459d-cacd-dd301264ed24"
   },
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_jIh849IRqA"
   },
   "source": [
    "We can also check the RAM usage again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SszHWIXIH2YS",
    "outputId": "3a9aa3f5-a743-4363-c693-c43e755fd873"
   },
   "outputs": [],
   "source": [
    "!free -m | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ir1K1wKGjJj"
   },
   "source": [
    "# Make the dataloader\n",
    "\n",
    "We then just create a normal torch Dataloader using the custom dataset class. We specify the filename and the name of the x and y datasets using `x_key` and `y_key`.\n",
    "\n",
    "Setting `num_workers=2` will use two threads to pre-load batches, this can help to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDM_GRD1CzWZ"
   },
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=H5Dataset(filename, x_key='time_series', y_key='targets'), \n",
    "    batch_size=1000, \n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BVLywcSHP6H"
   },
   "source": [
    "# Example training loop\n",
    "\n",
    "We can now use the dataloader in a normal training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inzIcJ7aDmHX",
    "outputId": "86005ef8-8288-48b9-fe95-2e8e16906a4c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, (x_batch, y_batch) in enumerate(loader):\n",
    "    # Do stuff here\n",
    "    a = 0\n",
    "    if not i % 10:\n",
    "        print(f'it: {i} / {len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5lNQav0IhsY"
   },
   "source": [
    "If we check the RAM usage again after running the loop, we see that it has barely changed despite having loaded all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcZ0S1lcFLav",
    "outputId": "efb1389d-91af-4b64-ed26-6a4cd1969ea2"
   },
   "outputs": [],
   "source": [
    "!free -m | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn90Ogt1OPhK"
   },
   "source": [
    "If you have seperate training and validation sets then you'll either need to have seperate files or customise `H5Dataset` to only use part of the file."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
